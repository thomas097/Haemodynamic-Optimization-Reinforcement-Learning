"""
Author:   Thomas Bellucci
Filename: train_dqn_roggeveen_et_al.py
Descr.:   Performs the training of a Dueling Double DQN model as described in
          (Roggeveen et al., 2021) using a handcrafted feature set.
Date:     01-10-2022
"""

import torch
import pandas as pd
from q_learning import DQN, fit_double_dqn
from importance_sampling import WIS


class OPECallback:
    """ This callback evaluates policy πe on a validation set of `valid_states`
        generated by a behavior policy πb and returns the WIS estimate of V^πe.
    """
    def __init__(self, behavior_policy_file, valid_states):
        # Load behavior policy that was used to sample validation set
        self._estimator = WIS(behavior_policy_file)
        self._states = torch.Tensor(valid_states.values)

    def __call__(self, policy):
        action_probs = policy.action_probs(self._states)
        wis = self._estimator(action_probs)
        return {'wis': wis}


if __name__ == '__main__':
    # State-space as described in (Roggeveen et al., 2021).
    STATE_COLUMNS = ['max_vp', 'total_iv_fluid', 'sirs_score', 'sofa_score', 'weight', 'ventilator', 'height',
                     'age', 'gender', 'heart_rate', 'temp', 'mean_bp', 'dias_bp', 'sys_bp', 'resp_rate', 'spo2',
                     'natrium', 'chloride', 'kalium', 'trombo', 'leu', 'anion_gap', 'aptt', 'art_ph', 'asat',
                     'alat', 'bicarbonaat', 'art_be', 'ion_ca', 'lactate', 'paco2', 'pao2', 'shock_index', 'hb',
                     'bilirubin', 'creatinine', 'inr', 'ureum', 'albumin', 'magnesium', 'calcium', 'pf_ratio',
                     'glucose', 'running_total_urine_output', 'total_urine_output', 'running_total_iv_fluid']

    # Training and validation data
    train_df = pd.read_csv('../preprocessing/datasets/mimic-iii/roggeveen/mimic-iii_train.csv')
    valid_df = pd.read_csv('../preprocessing/datasets/mimic-iii/roggeveen/mimic-iii_valid.csv')
    print('train.size = %s  valid.size = %s' % (len(train_df), len(valid_df)))

    # Evaluation callback using OPE
    ope_callback = OPECallback(behavior_policy_file='../ope/physician_policy/mimic-iii_valid_behavior_policy.csv',
                               valid_states=valid_df[STATE_COLUMNS])

    # Optimize DQN model
    dqn_model = DQN(state_dim=len(STATE_COLUMNS), num_actions=25, hidden_dims=(128, 128))

    fit_double_dqn(experiment='roggeveen_experiment',
                   policy=dqn_model,
                   states=train_df[STATE_COLUMNS],
                   actions=train_df.action,
                   rewards=train_df.reward,
                   episodes=train_df.icustay_id,
                   timesteps=train_df.timestep,
                   alpha=1e-4,
                   gamma=0.9,
                   tau=1e-2,
                   num_episodes=4000,
                   batch_size=8,
                   eval_func=ope_callback,
                   eval_after=100,
                   scheduler_gamma=0.95,
                   step_scheduler_after=200,
                   reward_clipping=15)
